# [ICCV 2025] HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models
[zhixiang wei](https://zxwei.site)<sup>1</sup>, guangting wang, et al. <br />
<sup>1</sup> University of Science of Techonology of China    <sup>2</sup> WeChat Vision, Tencent Inc.

Paper: https://arxiv.org/pdf/xxxx.xxxx.pdf

Page: https://zxwei.site/hqclip/

Demo: https://huggingface.co/spaces/zhixiangwei/hqclip

Datasets: https://huggingface.co/datasets/zhixiangwei/VLM-150M & https://huggingface.co/datasets/zhixiangwei/VLM-1B

* An efficient and effective image-text data generate pipeling, using Large Vision-Language Models (LVLMs) to generate multi-grained annotations.
* **VLM-1B** and **VLM-150M**: high quality image-text dataset with positive/negative and long/short text generated by state-of-the-art LVLMs.
* **HQ-CLIP**: a CLIP training paradigm that extends conventional contrastive learning by incorporating negative descriptions and short tags as additional supervised signals.
  
![teaser](https://github.com/user-attachments/assets/e700f75b-e0a5-4328-8466-6b496a4f971d)

## Citation
If you find our code or data helpful, please cite our paper:
```bibtex
xxx
```

## Acknowledgment
These works have greatly inspired us, providing us with codebases, data, and support. We thank their authors!
* [open_clip](https://github.com/mlfoundations/open_clip.git)
* [clip_benchmark](https://github.com/LAION-AI/CLIP_benchmark)
* [datacomp](https://github.com/mlfoundations/datacomp)
* [DFN](https://huggingface.co/collections/apple/dfn-models-data-659ecf85cebd98088a9d9a3b)
* [What If](https://huggingface.co/datasets/UCSC-VLAA/Recap-DataComp-1B)
