# [ICCV 2025] HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets 🚀

[![Paper PDF](https://img.shields.io/badge/📄_Paper-PDF-critical?logo=adobeacrobatreader)](https://arxiv.org/pdf/xxxx.xxxx.pdf)
[![Project Page](https://img.shields.io/badge/🌐_Project-Page-blue?logo=googlechrome)](https://zxwei.site/hqclip/)
[![Demo](https://img.shields.io/badge/🤗_Demo-HuggingFace_Spaces-yellow)](https://huggingface.co/spaces/zhixiangwei/hqclip)
[![Dataset VLM-150M](https://img.shields.io/badge/📀_Dataset-VLM--150M-brightgreen)](https://huggingface.co/datasets/zhixiangwei/VLM-150M)
[![Dataset VLM-1B](https://img.shields.io/badge/💿_Dataset-VLM--1B-success)](https://huggingface.co/datasets/zhixiangwei/VLM-1B)

**Authors**  
[Zhixiang Wei](https://zxwei.site)<sup>1</sup>, Guangting Wang et al.  
<sup>1</sup> University of Science and Technology of China  
<sup>2</sup> WeChat Vision, Tencent Inc.  

---

## 🔍 Key Contributions

* 🏭 **Efficient Data Generation Pipeline**  
Multi-grained annotation pipeline using Large Vision-Language Models (LVLMs)
* 🗂️ **High-Quality Image-Text Datasets**  
Generated by state-of-the-art LVLMs with positive/negative examples and rich text descriptions:
  - **[VLM-1B](https://huggingface.co/datasets/zhixiangwei/VLM-1B)**: Billion-scale dataset
  - **[VLM-150M](https://huggingface.co/datasets/zhixiangwei/VLM-150M)**: Curated 150M samples
* 🧠 **HQ-CLIP Training Framework**  
Novel CLIP training paradigm extending contrastive learning with:
  - Negative description supervision
  - Short tag augmentation

![Model Overview](https://github.com/user-attachments/assets/e700f75b-e0a5-4328-8466-6b496a4f971d)

---
## Model Zoo
|Model|Pretrained|ImageNet Top-1|DataComp Score|
|--|--|--|--|
|[CLIP-B-16](https://huggingface.co/zhixiangwei/vlm150m-hqclip-large-vitb16)|VLM-150M-Medium|70.6|58.6|
|[CLIP-L-14-CLIPA](https://huggingface.co/zhixiangwei/vlm1b-hqclip-xlarge-vitl14-clipa)|VLM-1B|78.6|63.8|
|[CLIP-L-14-OPENAI](https://huggingface.co/zhixiangwei/hqclip-openai-large-ft-vlm1b)|VLM-1B|76.5|63.7|

Recaption Model: [Qwen2VL](https://huggingface.co/zhixiangwei/qwen2-7b-full)

## Datasets
|Dataset|Samples|URL|
|--|--|--|
|VLM-150M|147M|https://huggingface.co/datasets/zhixiangwei/VLM-150M|
|VLM-1B|-|https://huggingface.co/datasets/zhixiangwei/VLM-1B|


## 📝 Citation
```bibtex
@InProceedings{Wei2025HQCLIP,
  title     = {HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets},
  author    = {Wei, Zhixiang and Wang, Guangting and et al.},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year      = {2025}
}
```

## 🙏 Acknowledgments
These works have greatly inspired us, providing us with codebases, data, and support. We thank their authors!
* [open_clip](https://github.com/mlfoundations/open_clip.git)
* [clip_benchmark](https://github.com/LAION-AI/CLIP_benchmark)
* [datacomp](https://github.com/mlfoundations/datacomp)
* [DFN](https://huggingface.co/collections/apple/dfn-models-data-659ecf85cebd98088a9d9a3b)
* [What If](https://huggingface.co/datasets/UCSC-VLAA/Recap-DataComp-1B)
