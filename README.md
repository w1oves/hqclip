# [ICCV 2025] HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets üöÄ

[![Paper PDF](https://img.shields.io/badge/üìÑ_Paper-PDF-critical?logo=adobeacrobatreader)](https://arxiv.org/abs/2507.22431)
[![Project Page](https://img.shields.io/badge/üåê_Project-Page-blue?logo=googlechrome)](https://zxwei.site/hqclip/)
[![Demo](https://img.shields.io/badge/ü§ó_Demo-HuggingFace_Spaces-yellow)](https://huggingface.co/spaces/zhixiangwei/hqclip)
[![Dataset VLM-150M](https://img.shields.io/badge/üìÄ_Dataset-VLM--150M-brightgreen)](https://huggingface.co/datasets/zhixiangwei/VLM-150M)
[![Dataset VLM-1B](https://img.shields.io/badge/üíø_Dataset-VLM--1B-success)](https://huggingface.co/datasets/zhixiangwei/VLM-1B)

**Authors**  
[Zhixiang Wei](https://zxwei.site)<sup>1</sup>, Guangting Wang et al.  
<sup>1</sup> University of Science and Technology of China  
<sup>2</sup> WeChat Vision, Tencent Inc.  

---

## üîç Key Contributions

* üè≠ **Efficient Data Generation Pipeline**  
Multi-grained annotation pipeline using Large Vision-Language Models (LVLMs)
* üóÇÔ∏è **High-Quality Image-Text Datasets**  
Generated by state-of-the-art LVLMs with positive/negative examples and rich text descriptions:
  - **[VLM-1B](https://huggingface.co/datasets/zhixiangwei/VLM-1B)**: Billion-scale dataset
  - **[VLM-150M](https://huggingface.co/datasets/zhixiangwei/VLM-150M)**: Curated 150M samples
* üß† **HQ-CLIP Training Framework**  
Novel CLIP training paradigm extending contrastive learning with:
  - Negative description supervision
  - Short tag augmentation

![Model Overview](https://github.com/user-attachments/assets/e700f75b-e0a5-4328-8466-6b496a4f971d)

---
## Model Zoo
|Model|Pretrained|ImageNet Top-1|DataComp Score|
|--|--|--|--|
|[CLIP-B-16](https://huggingface.co/zhixiangwei/vlm150m-hqclip-large-vitb16)|VLM-150M-Medium|70.6|58.6|
|[CLIP-L-14-CLIPA](https://huggingface.co/zhixiangwei/vlm1b-hqclip-xlarge-vitl14-clipa)|VLM-1B|78.6|63.8|
|[CLIP-L-14-OPENAI](https://huggingface.co/zhixiangwei/hqclip-openai-large-ft-vlm1b)|VLM-1B|76.5|63.7|

Recaption Model: [Qwen2VL](https://huggingface.co/zhixiangwei/qwen2-7b-full)

## Datasets
|Dataset|Samples|URL|
|--|--|--|
|VLM-150M|147M|https://huggingface.co/datasets/zhixiangwei/VLM-150M|
|VLM-1B|-|https://huggingface.co/datasets/zhixiangwei/VLM-1B|

## Dataset Usage Guide

### Preparation Steps
1. **(Optional) Download CommonPool Foundation Datasets**  
   Access CommonPool Large and XLarge versions via:  
   [CommonPool GitHub Repository](https://github.com/mlfoundations/datacomp#downloading-commonpool)

2. **Acquire DFN Base Datasets**  
   Download DFN Large and XLarge from:  
   [DFN Hugging Face Datasets](https://huggingface.co/datasets/apf1/datafilteringnetworks_2b)

3. **Download HQ-CLIP Datasets**  
   Obtain our enhanced datasets:
   - VLM-150M
   - VLM-1B

### Integration Instructions
Each JSON entry in VLM-150M and VLM-1B corresponds directly to a DFN dataset UID through matching filenames. To utilize our enhanced annotations:

- **Option 1: Direct Caption Replacement**  
  Substitute the original DFN captions with our generated annotations
  
- **Option 2: Dynamic Data Loading**  
  Modify the Open CLIP dataloader to load our annotations during training runtime

üîú Detailed implementation guidance will be published in future releases.


## Model Loading Instructions

Our uploaded weights are compatible with both `open_clip` and `huggingface transformers`. 

### For open_clip users:
```python
import open_clip

Initialize model with transforms

model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(
    'hf-hub:zhixiangwei/vlm150m-hqclip-large-vitb16'
)
tokenizer = open_clip.get_tokenizer(
    'hf-hub:zhixiangwei/vlm150m-hqclip-large-vitb16'
)
```


### For Hugging Face Transformers users:
```python
from transformers import AutoModel

Load model directly from hub

model = AutoModel.from_pretrained(
    'zhixiangwei/vlm150m-hqclip-large-vitb16'
)
```

## üìù Citation
```
@misc{hqclip,
      title={HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models}, 
      author={Zhixiang Wei and Guangting Wang and Xiaoxiao Ma and Ke Mei and Huaian Chen and Yi Jin and Fengyun Rao},
      year={2025},
      eprint={2507.22431},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2507.22431}, 
}
```

## üôè Acknowledgments
These works have greatly inspired us, providing us with codebases, data, and support. We thank their authors!
* [open_clip](https://github.com/mlfoundations/open_clip.git)
* [clip_benchmark](https://github.com/LAION-AI/CLIP_benchmark)
* [datacomp](https://github.com/mlfoundations/datacomp)
* [DFN](https://huggingface.co/collections/apple/dfn-models-data-659ecf85cebd98088a9d9a3b)
* [What If](https://huggingface.co/datasets/UCSC-VLAA/Recap-DataComp-1B)
