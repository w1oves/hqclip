# [ICCV 2025] HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets ğŸš€

[![Paper PDF](https://img.shields.io/badge/ğŸ“„_Paper-PDF-critical?logo=adobeacrobatreader)](https://arxiv.org/pdf/xxxx.xxxx.pdf)
[![Project Page](https://img.shields.io/badge/ğŸŒ_Project-Page-blue?logo=googlechrome)](https://zxwei.site/hqclip/)
[![Demo](https://img.shields.io/badge/ğŸ¤—_Demo-HuggingFace_Spaces-yellow)](https://huggingface.co/spaces/zhixiangwei/hqclip)
[![Dataset VLM-150M](https://img.shields.io/badge/ğŸ“€_Dataset-VLM--150M-brightgreen)](https://huggingface.co/datasets/zhixiangwei/VLM-150M)
[![Dataset VLM-1B](https://img.shields.io/badge/ğŸ’¿_Dataset-VLM--1B-success)](https://huggingface.co/datasets/zhixiangwei/VLM-1B)

**Authors**  
[Zhixiang Wei](https://zxwei.site)<sup>1</sup>, Guangting Wang et al.  
<sup>1</sup> University of Science and Technology of China  
<sup>2</sup> WeChat Vision, Tencent Inc.  

---

## ğŸ” Key Contributions

* ğŸ­ **Efficient Data Generation Pipeline**  
Multi-grained annotation pipeline using Large Vision-Language Models (LVLMs)
* ğŸ—‚ï¸ **High-Quality Image-Text Datasets**  
Generated by state-of-the-art LVLMs with positive/negative examples and rich text descriptions:
  - **[VLM-1B](https://huggingface.co/datasets/zhixiangwei/VLM-1B)**: Billion-scale dataset
  - **[VLM-150M](https://huggingface.co/datasets/zhixiangwei/VLM-150M)**: Curated 150M samples
* ğŸ§  **HQ-CLIP Training Framework**  
Novel CLIP training paradigm extending contrastive learning with:
  - Negative description supervision
  - Short tag augmentation

![Model Overview](https://github.com/user-attachments/assets/e700f75b-e0a5-4328-8466-6b496a4f971d)

---
## Model Zoo
|Model|Pretrained|ImageNet Top-1|DataComp Score|
|--|--|--|--|
|[CLIP-B-16](https://huggingface.co/zhixiangwei/vlm150m-hqclip-large-vitb16)|VLM-150M-Medium|70.6|58.6|
|[CLIP-L-14-CLIPA](https://huggingface.co/zhixiangwei/vlm1b-hqclip-xlarge-vitl14-clipa)|VLM-1B|78.6|63.8|
|[CLIP-L-14-OPENAI](https://huggingface.co/zhixiangwei/hqclip-openai-large-ft-vlm1b)|VLM-1B|76.5|63.7|

Recaption Model: [Qwen2VL](https://huggingface.co/zhixiangwei/qwen2-7b-full)

## Datasets
|Dataset|Samples|URL|
|--|--|--|
|VLM-150M|147M|https://huggingface.co/datasets/zhixiangwei/VLM-150M|
|VLM-1B|-|https://huggingface.co/datasets/zhixiangwei/VLM-1B|


## ğŸ“ Citation
```bibtex
@InProceedings{Wei2025HQCLIP,
  title     = {HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets},
  author    = {Wei, Zhixiang and Wang, Guangting and et al.},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year      = {2025}
}
```

## ğŸ™ Acknowledgments
These works have greatly inspired us, providing us with codebases, data, and support. We thank their authors!
* [open_clip](https://github.com/mlfoundations/open_clip.git)
* [clip_benchmark](https://github.com/LAION-AI/CLIP_benchmark)
* [datacomp](https://github.com/mlfoundations/datacomp)
* [DFN](https://huggingface.co/collections/apple/dfn-models-data-659ecf85cebd98088a9d9a3b)
* [What If](https://huggingface.co/datasets/UCSC-VLAA/Recap-DataComp-1B)
